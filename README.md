# ğŸ§  Projeto de AnÃ¡lise de Dados do Censo com Redes Neurais MLP e KNN

## ğŸ“‹ DescriÃ§Ã£o do Projeto

Este projeto implementa e compara dois algoritmos de Machine Learning para prever se a renda anual de um indivÃ­duo excede US$ 50.000/ano, utilizando dados do Censo dos Estados Unidos de 1994. O projeto combina **Multi-Layer Perceptron (MLP)** e **K-Nearest Neighbors (KNN)** para anÃ¡lise preditiva.

## ğŸ¯ Objetivo

Desenvolver modelos de classificaÃ§Ã£o capazes de prever com precisÃ£o se um indivÃ­duo tem renda anual superior a US$ 50.000 baseado em caracterÃ­sticas demogrÃ¡ficas, educacionais e profissionais.

## ğŸ“Š Dataset

### Fonte dos Dados
- **Origem**: UCI Machine Learning Repository
- **Fonte original**: Census Bureau Database (1994)
- **Tamanho**: 48.842 instÃ¢ncias (32.561 treino, 16.281 teste)
- **Classes**: 
  - `>50K`: 23.93% (classe minoritÃ¡ria)
  - `<=50K`: 76.07% (classe majoritÃ¡ria)

### CaracterÃ­sticas do Dataset
- **Tipo**: Mistura de variÃ¡veis contÃ­nuas e discretas
- **DimensÃµes**: 15 features
- **CaracterÃ­sticas principais**:
  - Idade, classe de trabalho, educaÃ§Ã£o
  - Estado civil, ocupaÃ§Ã£o, raÃ§a, sexo
  - Ganhos de capital, perdas de capital
  - Horas trabalhadas por semana, paÃ­s de origem

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

### 1. PrÃ©-processamento dos Dados
- **Limpeza**: RemoÃ§Ã£o de valores nulos e espaÃ§os em branco
- **CodificaÃ§Ã£o**: TransformaÃ§Ã£o de variÃ¡veis categÃ³ricas usando LabelEncoder
- **NormalizaÃ§Ã£o**: PadronizaÃ§Ã£o dos dados com StandardScaler
- **ReduÃ§Ã£o de dimensionalidade**: PCA mantendo 95% da variÃ¢ncia

### 2. DivisÃ£o dos Dados
- **Treino**: 80% dos dados
- **Teste**: 20% dos dados
- **ValidaÃ§Ã£o cruzada**: Para otimizaÃ§Ã£o de hiperparÃ¢metros

## ğŸ¤– Modelos Implementados

### Multi-Layer Perceptron (MLP)
**Arquitetura otimizada:**
- **Camada de entrada**: 15 features
- **Camada oculta**: 30 neurÃ´nios
- **Camada de saÃ­da**: 1 neurÃ´nio (classificaÃ§Ã£o binÃ¡ria)
- **FunÃ§Ã£o de ativaÃ§Ã£o**: tanh
- **Taxa de aprendizado**: 0.001
- **Solver**: Adam
- **MÃ¡ximo de iteraÃ§Ãµes**: 500-1000

**Vantagens:**
- Capacidade de capturar relaÃ§Ãµes nÃ£o-lineares complexas
- Boa generalizaÃ§Ã£o para padrÃµes complexos
- Melhor performance em mÃ©tricas crÃ­ticas

### K-Nearest Neighbors (KNN)
**ParÃ¢metros otimizados:**
- **NÃºmero de vizinhos (k)**: 3, 5, 7, 9, 11
- **Pesos**: uniform, distance
- **MÃ©trica de distÃ¢ncia**: euclidean, manhattan

**CaracterÃ­sticas:**
- Algoritmo baseado em instÃ¢ncias
- Simples de implementar e interpretar
- SensÃ­vel Ã  dimensionalidade dos dados

## ğŸ“ˆ Resultados e Performance

### MÃ©tricas de AvaliaÃ§Ã£o
- **AcurÃ¡cia**: Medida geral de acertos
- **PrecisÃ£o**: ProporÃ§Ã£o de prediÃ§Ãµes positivas corretas
- **Recall**: ProporÃ§Ã£o de casos positivos identificados
- **F1-Score**: MÃ©dia harmÃ´nica entre precisÃ£o e recall

### Performance dos Modelos

#### MLP
- **AcurÃ¡cia**: Superior em validaÃ§Ã£o cruzada e teste
- **Classe majoritÃ¡ria (<=50K)**: 
  - Recall: 93%
  - F1-Score: 91%
- **Classe minoritÃ¡ria (>50K)**:
  - F1-Score: 66%
  - Desafios com desbalanceamento de classes

#### KNN
- **AcurÃ¡cia**: Competitiva com MLP
- **Performance**: Boa em classes majoritÃ¡rias
- **LimitaÃ§Ãµes**: Menor capacidade de generalizaÃ§Ã£o

## ğŸ” AnÃ¡lise de Trade-offs

### MLP vs KNN
- **MLP**: Melhor em capturar padrÃµes complexos, mas requer mais dados e pode overfitting
- **KNN**: Simples e interpretÃ¡vel, mas sensÃ­vel Ã  dimensionalidade e custoso computacionalmente

### Arquitetura vs Performance
- **Simplicidade**: Arquitetura simples (1 camada, 30 neurÃ´nios) oferece boa acurÃ¡cia
- **Complexidade**: Mais camadas poderiam melhorar recall para classe minoritÃ¡ria
- **Balanceamento**: Trade-off entre simplicidade e capacidade de capturar padrÃµes complexos

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python**: Linguagem principal
- **Pandas**: ManipulaÃ§Ã£o e anÃ¡lise de dados
- **NumPy**: ComputaÃ§Ã£o numÃ©rica
- **Scikit-learn**: ImplementaÃ§Ã£o dos modelos e mÃ©tricas
- **Matplotlib/Seaborn**: VisualizaÃ§Ãµes
- **Jupyter Notebook**: Desenvolvimento e documentaÃ§Ã£o

## ğŸ“ Estrutura do Projeto

```
mlb/
â”œâ”€â”€ README.md                 # Este arquivo
â”œâ”€â”€ rede_neural.ipynb        # Notebook principal com implementaÃ§Ã£o
â”œâ”€â”€ data/                     # DiretÃ³rio de dados
â”‚   â””â”€â”€ adult/               # Dataset do Censo
â”‚       â”œâ”€â”€ adult.data       # Dados de treinamento
â”‚       â”œâ”€â”€ adult.test       # Dados de teste
â”‚       â”œâ”€â”€ adult.names      # DescriÃ§Ã£o das features
â”‚       â””â”€â”€ old.adult.names  # DocumentaÃ§Ã£o original
â””â”€â”€ .git/                     # Controle de versÃ£o
```

## ğŸš€ Como Executar

1. **Clone o repositÃ³rio:**
   ```bash
   git clone [URL_DO_REPOSITORIO]
   cd mlb
   ```

2. **Instale as dependÃªncias:**
   ```bash
   pip install pandas numpy scikit-learn matplotlib seaborn jupyter
   ```

3. **Execute o notebook:**
   ```bash
   jupyter notebook rede_neural.ipynb
   ```

## ğŸ“Š VisualizaÃ§Ãµes IncluÃ­das

- **AnÃ¡lise exploratÃ³ria dos dados**
- **DistribuiÃ§Ã£o das variÃ¡veis**
- **Matriz de correlaÃ§Ã£o**
- **Resultados de otimizaÃ§Ã£o de hiperparÃ¢metros**
- **ComparaÃ§Ã£o de performance entre modelos**
- **AnÃ¡lise de impacto dos parÃ¢metros**

## ğŸ“ ConclusÃµes

O **MLP** demonstrou ser o modelo superior para este problema especÃ­fico, oferecendo:
- Maior acurÃ¡cia geral
- Melhor capacidade de modelar relaÃ§Ãµes nÃ£o-lineares
- Performance superior em mÃ©tricas crÃ­ticas para a classe minoritÃ¡ria

O **KNN** apresentou performance competitiva, mas com limitaÃ§Ãµes em:
- Capacidade de generalizaÃ§Ã£o
- Sensibilidade Ã  dimensionalidade
- Custo computacional em prediÃ§Ã£o

## ğŸ”® PrÃ³ximos Passos

- **Balanceamento de classes**: Implementar tÃ©cnicas como SMOTE ou undersampling
- **Feature engineering**: Criar novas variÃ¡veis derivadas
- **Ensemble methods**: Combinar MLP e KNN para melhor performance
- **OtimizaÃ§Ã£o avanÃ§ada**: Usar tÃ©cnicas como Bayesian optimization
- **Deploy**: Implementar modelo em produÃ§Ã£o

## ğŸ‘¥ Autores

Desenvolvido como parte de um projeto acadÃªmico de Machine Learning.

## ğŸ“„ LicenÃ§a

Este projeto Ã© de uso educacional e acadÃªmico.

---

*Para mais detalhes sobre a implementaÃ§Ã£o, consulte o notebook `rede_neural.ipynb`*